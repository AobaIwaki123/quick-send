# ワークフロー詳細

## 🔄 全体の流れ

```
┌─────────────────────┐
│  1. データ収集      │  ← ユーザーがラベル付け
│   (Raycast)         │     「これはAI感がある」
└──────┬──────────────┘     「これは自然で良い」
       │
       ▼
┌─────────────────────┐
│  2. データ取得      │
│   (Memos API)       │  → collected_texts.json
└──────┬──────────────┘
       │
       ▼
┌─────────────────────┐
│  3. パターン学習    │  ← AIがユーザーの判断基準を学習
│   (ADK/Gemini)      │     「このような特徴がAI感」
└─────────────────────┘     「このような特徴が自然」
                            
                         → learned_patterns.json
                            - パターンの名前・説明
                            - 検出ルール
                            - 具体例
                            - 生成した例文
```

## 📝 各ステップの詳細

### Step 1: データ収集 (ユーザー主導)

**目的**: ユーザーの主観的判断を記録する

**手順**:
1. Webブラウザなどで文章を選択
2. `cmd + i` (設定したショートカット)
3. カテゴリ選択:
   - 👎 AI感: 「この文章、なんかAI臭い」
   - 👍 好き: 「この文章、読みやすい!」

**重要ポイント**:
- **判断基準はユーザー**: AIが決めるのではない
- **直感でOK**: 厳密な基準は不要
- **継続的に収集**: データが増えるほど精度向上

**保存場所**: Memos (http://localhost:5230)

---

### Step 2: データ取得

**スクリプト**: `scripts/collect_from_memos.py`

**実行**:
```bash
make collect
# または
python scripts/collect_from_memos.py
```

**処理内容**:
1. Memos APIに接続
2. ハッシュタグでフィルタ (`#ai_bad`, `#good`)
3. JSON形式で保存

**出力**: `data/collected_texts.json`
```json
[
  {
    "id": "memos/123",
    "text": "本日はお忙しい中...",
    "label": "ai_bad",
    "created_at": "2025-01-15T10:30:00Z"
  },
  ...
]
```

---

### Step 3: パターン学習 (AI主導)

**スクリプト**: `scripts/learn_patterns.py`

**実行**:
```bash
make learn-patterns
# または
python scripts/learn_patterns.py
```

**AIがやること**:

1. **データ分析**
   - AI感がある文章 vs 良い文章を比較
   - 統計的に有意な差を検出

2. **パターン抽出**
   - 語彙レベル: 頻出単語、フレーズ
   - 構文レベル: 文構造、接続詞
   - 意味レベル: 抽象度、感情表現

3. **ルール化**
   ```
   パターン: 過度な丁寧語
   検出ルール: 「させていただく」が1文中に2回以上
   例: "ご説明させていただきたく存じます"
   ```

4. **例文生成**
   - 各パターンを示す典型例を自動生成
   - ユーザーの理解を助ける

**プロンプト**: `prompts/pattern_learning.md`
```
以下のデータセットを分析し、ユーザーが「AI感がある」と
判断した文章の共通パターンを抽出してください。

### AI感がある文章 (ユーザーがラベル付け)
1. 本日はお忙しい中...
2. ご説明させていただきます...
...
```

**出力**: `data/learned_patterns.json`
```json
{
  "patterns": [
    {
      "id": "pattern_1",
      "name": "過度な丁寧語の使用",
      "description": "...",
      "strength": "strong",
      "detection_rule": "..."
    }
  ],
  "summary": {
    "total_patterns": 5,
    "strong_indicators": ["過度な丁寧語の使用"]
  }
}
```

## 🔄 継続的改善のサイクル

```
データ収集 → パターン学習 → 洞察獲得 → フィードバック
    ↑                                      ↓
    └──────────────────────────────────────┘
         新しいデータで再学習
```

1. **初期**: 少量のデータで学習開始
2. **分析**: learned_patterns.json を確認
3. **洞察**: 自分の「AI感」基準を理解
4. **追加収集**: 新しいパターンを見つけたらデータ追加
5. **再学習**: パターンを更新
6. **精度向上**: 繰り返しで言語化が改善

## 💡 Tips

### プロンプトの調整

パターン学習や検出の精度が低い場合:

1. `prompts/pattern_learning.md` を編集
   - 分析の観点を追加
   - 例の提示方法を変更

2. Git差分で変更を確認
   ```bash
   git diff prompts/pattern_learning.md
   ```

3. 再学習して効果を検証
   ```bash
   make learn-patterns
   ```

### データの質 vs 量

- **質**: 明確な判断基準でラベル付け
- **量**: 最低20-30件は欲しい
  - AI感: 10-15件
  - 良い文章: 10-15件

### パターンの検証

学習したパターンが妥当か確認:
```bash
# learned_patterns.json を確認
cat data/learned_patterns.json | jq '.patterns[].name'
```

おかしなパターンがあれば:
1. データを見直す
2. プロンプトを調整
3. 再学習