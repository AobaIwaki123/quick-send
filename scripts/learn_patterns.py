#!/usr/bin/env python3
"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ©ãƒ™ãƒ«ä»˜ã‘ã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯:
1. collected_texts.json ã‹ã‚‰ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
2. AIã«ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚’ä¾é ¼
3. å­¦ç¿’ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ patterns.json ã¨ã—ã¦ä¿å­˜
"""

import json
import os
from pathlib import Path
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = Path(__file__).parent.parent / "data"
PROMPTS_DIR = Path(__file__).parent.parent / "prompts"
DATASET_PATH = DATA_DIR / "collected_texts.json"
PATTERNS_PATH = DATA_DIR / "learned_patterns.json"


class PromptLoader:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    @staticmethod
    def load(filename: str) -> str:
        path = PROMPTS_DIR / filename
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    
    @staticmethod
    def load_with_vars(filename: str, **kwargs) -> str:
        template = PromptLoader.load(filename)
        return template.format(**kwargs)


def load_dataset() -> Dict[str, List[str]]:
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ©ãƒ™ãƒ«ã”ã¨ã«åˆ†é¡"""
    with open(DATASET_PATH, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    ai_bad_texts = [item["text"] for item in data if item["label"] == "ai_bad"]
    good_texts = [item["text"] for item in data if item["label"] == "good"]
    
    return {
        "ai_bad": ai_bad_texts,
        "good": good_texts
    }


def format_examples(texts: List[str], max_examples: int = 20) -> str:
    """ä¾‹æ–‡ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not texts:
        return "(ãƒ‡ãƒ¼ã‚¿ãªã—)"
    
    examples = texts[:max_examples]
    formatted = []
    for i, text in enumerate(examples, 1):
        formatted.append(f"{i}. {text}")
    
    if len(texts) > max_examples:
        formatted.append(f"\n... ä»– {len(texts) - max_examples} ä»¶")
    
    return "\n".join(formatted)


def learn_patterns(dataset: Dict[str, List[str]]) -> Dict:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
    
    å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã“ã“ã§ADKã‚„Vertex AIã®APIã‚’å‘¼ã³å‡ºã™
    """
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™
    prompt = PromptLoader.load_with_vars(
        "pattern_learning.md",
        ai_bad_examples=format_examples(dataset["ai_bad"]),
        good_examples=format_examples(dataset["good"])
    )
    
    print("ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    print(f"   AIæ„ŸãŒã‚ã‚‹æ–‡ç« : {len(dataset['ai_bad'])} ä»¶")
    print(f"   è‰¯ã„æ–‡ç« : {len(dataset['good'])} ä»¶")
    print()
    
    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ã“ã“ã§AIã‚’å‘¼ã³å‡ºã™
    # response = call_ai_api(prompt)
    # patterns = json.loads(response)
    
    # ãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    patterns = {
        "patterns": [
            {
                "id": "pattern_1",
                "name": "éåº¦ãªä¸å¯§èªã®ä½¿ç”¨",
                "description": "ã€Œã€œã•ã›ã¦ã„ãŸã ãã€ã€Œã€œã§ã”ã–ã„ã¾ã™ã€ãªã©ã®ä¸å¯§èªãŒé »å‡ºã—ã€ä¸è‡ªç„¶ã«ç¤¼å„€æ­£ã—ã„å°è±¡ã‚’ä¸ãˆã‚‹",
                "strength": "strong",
                "frequency": 0.75,
                "examples_from_data": [
                    "æœ¬æ—¥ã¯ãŠå¿™ã—ã„ä¸­ã”å‚åŠ ã„ãŸã ãã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™",
                    "ã”èª¬æ˜ã•ã›ã¦ã„ãŸã ãã¾ã™"
                ],
                "synthetic_examples": [
                    "ã“ã¡ã‚‰ã®è³‡æ–™ã‚’ã”è¦§ã„ãŸã ã‘ã¾ã™ã§ã—ã‚‡ã†ã‹",
                    "ã”ç¢ºèªã•ã›ã¦ã„ãŸã ããŸãå­˜ã˜ã¾ã™"
                ],
                "detection_rule": "ã€Œã•ã›ã¦ã„ãŸã ãã€ãŒ1æ–‡ä¸­ã«2å›ä»¥ä¸Šã€ã¾ãŸã¯æ–‡ç« å…¨ä½“ã§é »å‡ºã™ã‚‹å ´åˆ"
            },
            {
                "id": "pattern_2",
                "name": "æ©Ÿæ¢°çš„ãªç®‡æ¡æ›¸ãæ§‹é€ ",
                "description": "ã€Œã¾ãšã€ã€Œæ¬¡ã«ã€ã€Œæœ€å¾Œã«ã€ã®å®šå‹çš„ãªå±•é–‹ãŒå¤šç”¨ã•ã‚Œã‚‹",
                "strength": "medium",
                "frequency": 0.60,
                "examples_from_data": [
                    "ã¾ãšã€èƒŒæ™¯ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚æ¬¡ã«ã€å…·ä½“çš„ãªæ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚æœ€å¾Œã«ã¾ã¨ã‚ã¾ã™ã€‚"
                ],
                "synthetic_examples": [
                    "ç¬¬ä¸€ã«ã€œã€ç¬¬äºŒã«ã€œã€ç¬¬ä¸‰ã«ã€œ",
                    "1ã¤ç›®ã¯ã€œã€2ã¤ç›®ã¯ã€œã€3ã¤ç›®ã¯ã€œ"
                ],
                "detection_rule": "ã€Œã¾ãš/æ¬¡ã«/æœ€å¾Œã«ã€ã¾ãŸã¯ç•ªå·ä»˜ã‘ãŒé€£ç¶šã—ã¦å‡ºç¾"
            }
        ],
        "summary": {
            "total_patterns": 2,
            "strong_indicators": ["éåº¦ãªä¸å¯§èªã®ä½¿ç”¨"],
            "common_features": {
                "lexical": ["ã•ã›ã¦ã„ãŸã ã", "ã”ã–ã„ã¾ã™", "å­˜ã˜ã¾ã™"],
                "syntactic": ["ç®‡æ¡æ›¸ã", "ç•ªå·ä»˜ã‘ãƒªã‚¹ãƒˆ"],
                "semantic": ["éåº¦ã«å½¢å¼çš„", "å€‹äººçš„è¦–ç‚¹ã®æ¬ å¦‚"]
            }
        },
        "insights": [
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å½¢å¼çš„ã™ãã‚‹æ–‡ç« ã‚’AIæ„ŸãŒã‚ã‚‹ã¨åˆ¤æ–­ã™ã‚‹å‚¾å‘",
            "å…·ä½“ä¾‹ã‚„å€‹äººçš„ãªè¦–ç‚¹ãŒã‚ã‚‹æ–‡ç« ã¯ã€Œè‰¯ã„ã€ã¨è©•ä¾¡ã•ã‚Œã‚‹"
        ]
    }
    
    return patterns


def save_patterns(patterns: Dict, output_path: Path = PATTERNS_PATH):
    """å­¦ç¿’ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(patterns, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")


def print_summary(patterns: Dict):
    """å­¦ç¿’çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    print("\n" + "="*60)
    print("ğŸ“Š å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    
    summary = patterns.get("summary", {})
    print(f"\næŠ½å‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {summary.get('total_patterns', 0)}")
    
    print("\nã€å¼·ã„æŒ‡æ¨™ã€‘")
    for indicator in summary.get("strong_indicators", []):
        print(f"  - {indicator}")
    
    print("\nã€å…±é€šç‰¹å¾´ã€‘")
    features = summary.get("common_features", {})
    for category, items in features.items():
        print(f"  {category}: {', '.join(items)}")
    
    print("\nã€æ´å¯Ÿã€‘")
    for insight in patterns.get("insights", []):
        print(f"  â€¢ {insight}")
    
    print("\n" + "="*60)


def main():
    print("ğŸ” ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™\n")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    print("ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    dataset = load_dataset()
    
    if not dataset["ai_bad"] and not dataset["good"]:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   å…ˆã« collect_from_memos.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
    print("\nğŸ¤– ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™...")
    patterns = learn_patterns(dataset)
    
    # çµæœä¿å­˜
    save_patterns(patterns)
    
    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print_summary(patterns)
    
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. data/learned_patterns.json ã‚’ç¢ºèª")
    print("  2. python src/agent.py ã§æ–°ã—ã„æ–‡ç« ã‚’è©•ä¾¡")


if __name__ == "__main__":
    main()